- 티처블 머신 실습
<img width="1062" alt="우삼겹3" src="https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/155953972/60e4da93-199c-4757-b5d6-3b26f4e418bd">
<img width="982" alt="차돌1" src="https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/155953972/0b30ba58-d1ca-4bec-9b58-d4f184569182">
<blockquote> 차돌박이와 우삼겹을 학습시켰다. 두 부위는 고기에 관심이 많지 않다면 대부분의 사람들도 구분하기 어려워하는 부위라 과연 사진 몇 장 만으로 티쳐블머신이 둘을 구분할 수 있는지가 궁금했기 때문이다. 우삼겹과 차돌 모두 각각 10장의 이미지를 학습시켰고 신기하게도 티쳐블머신은 이 둘을 어느정도 구분해냈다. 둘의 비율이 비슷하게 나온 것도 있었지만 완전히 잘못된 결과를 도출해낸 경우는 없었다.
</blockquote>
<br><br>

- 대패삼겹살과의 비교
<img width="973" alt="대패2" src="https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/155953972/0e4a24f4-5620-40c8-99cb-bae9d953082f">
<blockquote> 차돌박이, 우삼겹과 비슷한 또 다른 고기인 대패삼겹살을 넣고 둘 중 어디에 더 가까운지 비교해보았다. 내 예상으로는 우삼겹이 더 비슷하게 나올거라고 생각했지만 의외로 차돌박이라고 인식한 비율이 훨씬 높았다.
</blockquote>
<br><br>

-epoch, batch size란?
1. epoch: 전체 데이터 셋이 신경망을 통과한 횟수. 즉, 모든 데이터를 학습한 횟수
  <blockquote>
    if epoch:2면? 모든 데이터를 2번 학습했다는 뜻 (문제집으로 비교하면 한 문제집을 2회독 한 느낌)<br>
    epoch가 너무 많으면 Overfitting, 너무 적으면 Underfitting이 일어날 가능성이 높아진다.
  </blockquote>
2. batch size: 전체 데이터 셋을 여러 개의 작은 소그룹으로 나누었을 때 한 그룹에 속해있는 데이터의 수
   <blockquote>
     if batch size = 40이면? 하나의 소그룹에 들어있는 데이터의 개수가 40개라는 뜻.<br>
     batch size가 너무 크면? 한 번에 처리해야할 데이터의 양이 너무 많아져서 학습속도가 느려지고 메모리 부족 문제가 발생한다.<br>
     batch size가 너무 작으면? 너무 적은 양의 데이터로 인해 가중치가 자주 업데이트되어 학습이 불안정해진다.
   </blockquote>
<br><br>

-이번 활동을 통해 알게된 점
<blockquote>
  머신러닝에 대한 내용은 들어보기만 했지 내가 실제로 학습을 시켜본 것은 이번이 처음이라 신기했다. 또 epoch와 batch size에 대해 알아보면서 각 단어들의 의미 뿐만 아니라 머신러닝의 기초적인 내용들(ex: 기계가 학습하는 과정, 그 과정에서 왜 epoch, batch size가 필요한지.)을 알아볼 수 있는 좋은 기회가 된 것 같다.
</blockquote>
