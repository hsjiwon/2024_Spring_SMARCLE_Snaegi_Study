# 1주차 : 티쳐블 학습
### (1) 다른 나라, 같은 머리
<img width="1100" alt="image" src="https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/162942977/a63ccc2c-6aac-4a9a-857e-0a71a13def1d"> 
-치와와랑 머핀이랑 비교할려고 했는데 생각보다 안될 것 같기도 하고 이미 나와 있는 거라서 기영이랑 심슨을 <br>
-사진을 찾으면서 기영이 심슨중 누굴 더 잔디라고 생각하는지 궁금했다.<br>
-일단 생각만 해두고 사진을 넣어봤다... <br> 
-처음엔 3개씩 넣어서 해봤는데 인식을 잘 못했다 그래서 개수를 13개씩 늘려서 해봤는데 인식률이 확실히 높아졌다. <br>

- *(자료빨이 역시 중요한 것 같다)* -
***
### (2) 그래서 비교해봤다!! 누가 더 잔디 같을까???
-사진을 넣어보면서 생각해봤던 걸 바로 실행시켜봤다 ㅋㅋㅋㅋ<br>
.<br>
<img width="1100" alt="image" src="https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/162942977/894c3588-0433-4bc4-bedb-7934ddd322ee"><br>
-구글 티처블 생각엔 기영이가 더 잔디 같다고 생각했다 ㅋㅋㅋㅋㅋㅋㅋ

*(다음엔 잔디랑 다른 물체를 학습시켜서 기영이를 스캔해봐야겠다 ㅋㅋㅋ)*
***
### (3) 티쳐블 머신을 학습하면서 느낀점.
-고등학교 동아리때 이걸 주제로 한번 애들이랑 활동해본 적이 있었는데 그때 생각도 나고 그리고 지금은 그래도 업데이트가 되어서 그런지 그때보다 더 스캔이 잘 되었던 것 같아 순조롭게 진행되었던 것 같다.<br>

***
### epoch 와 batch size는 무엇인가?

- epoch: 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass(전처리와 후처리) 과정을 거친 것을 말함. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태-- 40 epochs = 40이라면 전체 데이터를 40번 사용해서 학습을 거치는 것이다.
- epoch값이 너무 작으면 Underfitting(모델이 학습을 할때 발생하는 오차율이 너무 커짐), 너무 많으면 Overfitting(변수에 과도하게 민감하여 반대로 오차율이 커짐 + 학습데이터가 많으므로 용량역시 커짐)이 발생할 확률이 높다.
- batch size: batch size는 한 번의 batch마다 주는 데이터 샘플의 size.
- 여기서 batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻하며 iteration(반복)는 epoch를 나누어서 실행하는 횟수라고 생각하면 된다.
 - batch size가 너무 큰 경우 한 번에 처리해야 할 데이터의 양이 너무 많아져 학습 속도가 느려지고, 메모리 부족 문제가 발생할 수 있다. 반면 batch size가 너무 작은 경우 적은 데이터로 가중치가 너무 자주 업데이트되어 훈련이 불안정해진다.

