- 티쳐블 머신 실습

![image](https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/138707859/ac6b4ee0-00cc-4de7-8ef8-bfd109baf8c0)
![image](https://github.com/sejongsmarcle/2024_Spring_SMARCLE_Snaegi_Study/assets/138707859/4e6cf70a-0508-4b23-acf1-4bf339fae05c)

> 황제성과 샘 스미스를 주제로 학습시켜 보았다. 그리고 황제성이 샘스미스의 언홀리 모습을 따라한 것을 분류할 수 있을까 궁금하여 시도를 해 보았는데
많이 비슷해서 그런가 제대로 분류하지 못하는 모습을 볼 수 있었다.

- epoch, batch size개념
1. epoch: 모든 데이터셋을 학습하는 횟수 = 전체 데이터 셋이 신경망을 통과한 횟수
2. batch size: 전체 데이터 셋을 여러 작은 그룹으로 나누었을때, 하나의 소그룹에 속하는 데이터 수
3. 정리:
   기계를 학습시킬때 사용하는 데이터의 양은 굉장히 많기때문에 한번에 모든 데이터를 학습시키는것은 무리가 있다.
   만약 가능하더라도 컴퓨터의 성능에 치명적인 영향을 줄것이다. 그렇기 때문에 머신러닝에서 최적화가 필요하고 이런경우 일반적으로 여러번의 학습과정을 거친다.
   한번 학습할때 batch size를 정하여 데이터의 크기를 나누는 방식으로 컴퓨터의 무리를 덜 주고, epoch를 몇번할 지 정하여 성능을 향상시킬 수 있다.

- 알게된 점
  > 티쳐블 머신은 사실 이전에 사용한 적이 있었는데, 오랜시간동안 사용하지않아 상당수 부분을 까먹고 있었다. 하지만 이렇게 티쳐블 머신을 사용해보며 잊어버렸던 개념들 및 감을 상기시킬 수 있었다.  
    새롭게 알게 된 점은 epoch와 batch size에 관한 개념들이다. 이전에는 정확한 이유나 개념들 없이 그저 머신러닝에서 최적화를 할때 "뭐 여러번 나눠서 학습하겠지~" & " 한번만 학습하진 않을테고 대충 여러번 하겠지~"
    이렇게 생각을 했었다. 하지만 이렇게 epoch&batch size의 개념들에 대해 찾아보며 정확한 단어와 이러한 개념들이 왜 생겨났는지에 대한 이유를 상세하게 알게된 것 같다.
